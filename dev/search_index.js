var documenterSearchIndex = {"docs":
[{"location":"ebayes_samples/#EBayes-sample-types-1","page":"Empirical Bayes Samples","title":"EBayes sample types","text":"","category":"section"},{"location":"ebayes_samples/#","page":"Empirical Bayes Samples","title":"Empirical Bayes Samples","text":"NormalSample","category":"page"},{"location":"ebayes_samples/#EBayes.NormalSample","page":"Empirical Bayes Samples","title":"EBayes.NormalSample","text":"NormalSample(Z,σ)\n\nA observed sample Z drawn from a Normal distribution with known variance sigma^2  0.\n\nZ sim mathcalN(mu sigma^2)\n\nmu is assumed unknown. The type above is used when the sample Z is to be used for estimation or inference of mu.\n\nNormalSample(0.5, 1.0)          #Z=0.5, σ=1\n\n\n\n\n\n","category":"type"},{"location":"prior_distributions/#Empirical-Bayes-Priors-1","page":"Example Priors","title":"Empirical Bayes Priors","text":"","category":"section"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"import Pkg\nPkg.add(\"StatsPlots\")\nPkg.add(\"GR\")\nENV[\"GKSwstype\"] = \"100\"\nusing EBayes\nusing StatsPlots\nusing InteractiveUtils\ngr()","category":"page"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"EBayes.AshSpiky","category":"page"},{"location":"prior_distributions/#EBayes.AshSpiky","page":"Example Priors","title":"EBayes.AshSpiky","text":"AshSpiky, AshNearnormal, AshFlattop, AshSkew, AshBignormal, AshBimodal\n\nEmpirical Bayes priors that are used in the simulations of:\n\nStephens, M., 2017. False discovery rates: a new deal. Biostatistics, 18(2), pp.275-294.\n\n\n\n\n\n","category":"type"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"ash_prs = [T() for  T in subtypes(EBayes.AshPrior)]\nplot(plot.(ash_prs)...)\nsavefig(\"plot_ash_priors.png\") # hide","category":"page"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"(Image: )","category":"page"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"EBayes.IWUnimod","category":"page"},{"location":"prior_distributions/#EBayes.IWUnimod","page":"Example Priors","title":"EBayes.IWUnimod","text":"IWUnimod, IWBimod\n\nEmpirical Bayes priors that are used in the simulations of:\n\nIgnatiadis, N. and Wager, S., 2019. Bias-aware confidence intervals for empirical Bayes analysis. arXiv preprint arXiv:1902.02774.\n\n\n\n\n\n","category":"type"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"plot(plot.([EBayes.IWUnimod(), EBayes.IWBimod()])..., size=(600,300))\nsavefig(\"plot_iw_priors.png\") # hide","category":"page"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"(Image: )","category":"page"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"EBayes.EfronUnifNormal","category":"page"},{"location":"prior_distributions/#EBayes.EfronUnifNormal","page":"Example Priors","title":"EBayes.EfronUnifNormal","text":"EfronUnifNormal\n\nEmpirical Bayes priors that are used in the simulations of:\n\nEfron, B., 2016. Empirical Bayes deconvolution estimates. Biometrika, 103(1), pp.1-20.\n\n\n\n\n\n","category":"type"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"plot(EBayes.EfronUnifNormal(), size=(600,300))\nsavefig(\"plot_efron_prior.png\") # hide","category":"page"},{"location":"prior_distributions/#","page":"Example Priors","title":"Example Priors","text":"(Image: )","category":"page"},{"location":"#EBayes.jl-1","page":"Home","title":"EBayes.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Depth=3","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A package for empirical Bayes (EB) estimation in Julia. Currently the main functionality is estimation in the Gaussian compound decision problem in which we observe:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Z_i sim mathcalN(mu_i sigma_i^2) i=1dotscn","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Z_i is observed, sigma_i^2 is assumed to be known and mu_i is unknown. The goal is to use all Z_i and potential covariates X_i to design estimators hatmu_i of mu_i such that sum_i mathbbEleft(mu_i - hatmu_i)^2right is small.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A longer term goal of this package is to provide a unified interface for EB method development and applications.","category":"page"},{"location":"#Getting-started-1","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"#SURE-EB-estimator-1","page":"Home","title":"SURE EB estimator","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Let us generate toy data:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"import Pkg\nPkg.add(\"MLJ\")\nPkg.add(\"MLJBase\")\nPkg.add(\"MLJModels\")","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using Random\nRandom.seed!(1)\n\nn = 10000\nσs = fill(1.0, n)\nμs = randn(n) .* σs\nZs = μs .+ randn(n)\nnothing  # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We first check the mean squared error if we try to estimate mu_i by Z_i:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using StatsBase\nmean( (μs - Zs).^2 )","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Instead let us use the Normal SURE method of Xie, Kou, and Brown (2012), which has been implemented in this package. To do this, we will first need to wrap the Zs and σs as NormalSamples.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using EBayes\nss = NormalSamples(Zs, σs)\nsure_fit = fit(Normal(), SURE(), ss)\nsure_pred = predict(sure_fit)\n\nmean( (μs - sure_pred).^2 )","category":"page"},{"location":"#EB-with-covariates:-EBayesCrossFit-1","page":"Home","title":"EB with covariates: EBayesCrossFit","text":"","category":"section"},{"location":"#Regression-estimators-1","page":"Home","title":"Regression estimators","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Now let us consider the case that we also have contextual side information about each unit i, i.e., we have covariates X_i for each i that may be informative about the mu_i. The difference compared to Z_i is that we do not  make any probabilistic assumptions about how X_i is related to mu_i (while we know that Z_i is unbiased for mu_i).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"If the X_i's capture mu_i perfectly, then a powerful machine learning method will be able to learn the relationship if we regress Z_i sim X_i. For this task we can use MLJ.jl, which provides a unified framework for working with machine learning methods.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Continuing with our toy problem, let us simulate two-dimensional covariates; the first dimension is informative for mu_i, while the second is not.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using MLJ, MLJBase\n\nXs1 = μs .+ randn(n)\nXs2 = randn(n)\nXs = table([Xs1 Xs2])\nnothing  # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We now use MLJ to fit a model (here just ordinary least squares linear regression) and then use the model to predict the mu_i's:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"\nlin_mlj = MLJ.@load LinearRegressor pkg=GLM\n\nlin_mach = fit!(machine(lin_mlj, Xs, Zs), verbosity=0)\nlin_preds = predict_mean(lin_mach)\n\nmean((lin_preds .- μs).^2)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"In this example, the regression has almost the same mean squared as the SURE method.","category":"page"},{"location":"#EBayesCrossFit-1","page":"Home","title":"EBayesCrossFit","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"We have seen two strong baselines: The empirical Bayes (SURE) method that utilizes the information in the Z_i's by accounting for their known sampling distributions and the regression (ML) method that utilizes the information in the X_i's. The EBayesCrossFit method (Ignatiadis and Wager, 2019) synthesizes both sources of information through the empirical Bayes principle and by simultaneously leveraging any (black-box) ML method.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Let us apply the EBayesCrossFit method in conjuction with the linear regression we used previously:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"\nebcf_lin = EBayesCrossFit(lin_mlj) # EBCF with linear regression\nebcf_fit = fit(ebcf_lin, Xs, ss)   # fit EBCF\nebcf_preds = predict(ebcf_fit)     # gather EBCF predictions\n\nmean((ebcf_preds .- μs).^2)        # evaluate EBCF","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We see that indeed the EBayesCrossFit method outperforms both baselines.","category":"page"}]
}
